# -*- coding: utf-8 -*-
"""Jeopardy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12OmHT3qLn2cDbDz8fbQWiYLsVVVgWpqa

# Unstructured Data Analysis
## Jeopardy Questions Dataset
### Jules Capaldo, Peter Christie, Megan Crawford, Clarinsa Djaja

#### Set Up
"""

import pandas as pd
import numpy as np
import nltk
from nltk import FreqDist
from nltk.tokenize import sent_tokenize, word_tokenize
import matplotlib.pyplot as plt

nltk.download('punkt')

nltk.download("book")

jeopardy_1st_half = pd.read_csv("https://raw.githubusercontent.com/peter-christie/uda_jeopardy/main/newer_jeopardy_questions.csv")

jeopardy_2nd_half = pd.read_csv("https://raw.githubusercontent.com/peter-christie/uda_jeopardy/main/older_jeopardy_questions.csv")

jeopardy_1st_half.shape

jeopardy_2nd_half.shape

jeopardy = pd.concat([jeopardy_1st_half, jeopardy_2nd_half], ignore_index = True)

# Clear up memory, data frames no longer needed
del(jeopardy_1st_half)
del(jeopardy_2nd_half)

jeopardy.shape

jeopardy.head()

jeopardy.drop(["Unnamed: 0"], axis = 1, inplace = True)

jeopardy.head()

jeopardy['Round'].value_counts()

jeopardy.dtypes

#jeopardy['Question'] = jeopardy['Question'].apply(word_tokenize)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""#### Top Categories"""

category_type = jeopardy['Category'].value_counts()

#category_type

print("The top ten categories overall are: ")
category_type[0:10]

category_type_r1 = jeopardy[jeopardy['Round'] == 'Jeopardy!']['Category'].value_counts()

#category_type_r1

print("The top ten categories for the 'Jeopardy!' round are: ")
category_type_r1[0:10]

category_type_r2 = jeopardy[jeopardy['Round'] == 'Double Jeopardy!']['Category'].value_counts()

#category_type_r2

print("The top ten categories for the 'Double Jeopardy!' round are: ")
category_type_r2[0:10]

running_total = np.cumsum(category_type)
running_total = pd.DataFrame(running_total)
running_total = running_total.reset_index()

running_total.head()

running_total.shape

figure, axes = plt.subplots()

running_total.plot(ax = axes, kind='line', color = 'blue')

"""Here we can see that there are approximately 200,000 questions in the data set. The graph above shows the running total of questions grouped by the category type. Based on this we can see that relatively few categories account for a disproportionate number of questions. For example, 50,000 questions are taken from only about 575 different categories. Given that there are over 25,000 unique categories we will reduce our data set to only the categories that appear the most frequently."""

# Select only the top 100 categories
category_type = category_type[0:100]

common_categories = list(category_type.index)
## Create top lists for each round

common_categories[:10]

common_categories

category_type[99]

"""Here we are selecting categories which are in the top 100 most common categories. The hundredth most common category has 133 questions asked from it. Since there are five questions per category, this would indicate that all of these categories have appeared at least 27 times over the years."""

jeopardy.head()

reduced = jeopardy[jeopardy["Category"].isin(common_categories)]
reduced.head()

jeopardy['Air.Date'] = pd.to_datetime(jeopardy['Air.Date'])

jeopardy.head()

print(reduced.shape)

# Clear up memory, full data frame no longer needed
#del(jeopardy)

corpus = reduced['Question'].tolist()

corpus[0:2]

"""#### Term Frequency Calculations for the Questions.
Which terms appear the most for questions that belong to the top 100 categories?
"""

# Term Frequency Calculations
def term_frequency(corpus):
  combined_text = []

  for doc in corpus:
    combined_text.extend(word_tokenize(doc))

  vocabulary = set(combined_text)

  tokens_to_idx = {}
  i = 0
  for token in vocabulary:
    tokens_to_idx[token] = i
    i += 1

  result = np.zeros((len(corpus), len(vocabulary)), dtype = np.int32)

  for j in range(len(corpus)): #tracks which document we are in
      doc = word_tokenize(corpus[j]) #tokenize the
      for token in doc:
          token_idx = tokens_to_idx[token]
          result[j, token_idx] += 1

  return vocabulary, result

vocab, res = term_frequency(corpus)

vocab = list(vocab)

len(vocab)

res.shape

"""The code above generates the term frequency calculations for the `reduced` data frame. This matrix is quite large as can be seen in the cell directly above. Further, this matrix is extremely sparce."""

from nltk.corpus import stopwords

stopwords_en = stopwords.words("english")

import string
stopwords_en.extend(string.punctuation)
custom_stopword_list = ["``", "--", "''", "...", "'s"]
stopwords_en.extend(custom_stopword_list)

total_corpus = []
for doc in corpus:
  question_tokenized = word_tokenize(doc)
  total_corpus.append(question_tokenized)

filtered_corpus = []

for doc in total_corpus:
  for word in doc:
    if word.lower() not in stopwords_en:
      filtered_corpus.append(word)

"""The cell above creates a new corpus which does not contain stop words. The original corpus needed to be tokenized since it previously was one string per question."""

fdist1 = FreqDist(filtered_corpus)
fdist1.plot(25, cumulative=False)
print(fdist1.most_common(50))

"""Most common terms in the common categories. Tend to due with location quesitons.

#### Answer Term Frequency
Which answers appear the most for the entire data set?
"""

common_answers = jeopardy['Answer'].value_counts()

common_answers[0:20]

common_answers = list(common_answers.index)
common_answers = common_answers[0:20]

"""Of the twenty most common answers in the Jeopardy! dataset, we can see that all of them are proper noun locations. Predominantly, these are countries, but cities and a few states are present in the top twenty answers.

#### Dispersion plots for the most common categories.
"""

# Sort in chronological order
jeopardy.sort_values('Show.Number', inplace = True)

show_cat = jeopardy.groupby(['Show.Number', 'Category']).size()

show_cat = pd.DataFrame(show_cat)
# Change to data frame from series

show_cat = show_cat.reset_index()
show_cat.head()
# Reset index so can use the to list function

category = show_cat['Category'].tolist()
category[0:5]

category = nltk.Text(category)

category.dispersion_plot(common_categories[:10])

"""Above we have plotted the dispersion plots for the categories. It should be noted that the x-axis corresponds roughly to the show number, so it is fairly chronological. It appears that the early shows tended to reuse the same categories fairly frequently while the more recent ones perhaps have used different names for the categories or maybe have moved in a different direction entirely.

#### TF-IDF Scores on Questions grouped by year of air date
"""

# Group by year and combine the questions into one cell
show_groups = jeopardy.groupby(jeopardy['Air.Date'].dt.year)['Question'].unique().apply(' '.join).reset_index()

# if a by show grouping is desired

#show_groups = jeopardy.groupby('Show.Number')['Question'].unique().apply(' '.join).reset_index()

show_groups[0:2]

corpus2 = show_groups['Question'].tolist()

corpus2[0:2]

print(type(corpus2))
print(type(corpus2[0]))

from sklearn.feature_extraction.text import CountVectorizer

def tfidf_vectorizer(corpus):
    cvect = CountVectorizer(lowercase = True, tokenizer=nltk.word_tokenize, stop_words=stopwords_en, max_features=10000)
    count_matrix = cvect.fit_transform(corpus)
    tokens = cvect.get_feature_names()

    count_matrix = pd.DataFrame(count_matrix.todense())

    df_vect = count_matrix.astype(bool).sum(axis=0)
    df_vect = np.log(len(corpus) / df_vect)

    return tokens, np.array(count_matrix * df_vect)

tokens, tfidfvals = tfidf_vectorizer(corpus2)

# Create vocabulary and cross reference dictonaries

# build our idx_to_token dictionary
idx_to_tokens = {}
tokens_to_idx = {}

for i in range(len(tokens)):
  token = tokens[i]
  tokens_to_idx[token] = i
  idx_to_tokens[i] = token

tfidfvals.shape
# Shows the number of shows by 10,000 possible features.

"""The above code combines all of the questions from each show into one cell per year. These cells were treated as the documents and each show formed a corpus. The tf-idf values were then calculated."""

for i in range(len(corpus2)):
    idx = np.argsort(tfidfvals[i])
    idx = idx[::-1]

    print("Year: ", 1984+i)
    for j in idx[0:5]:
        print(tokens[j])
    print("\n")

"""This shows the five most important terms which appeared in quesitons for each year. Interestingly, year values, eg, 2000, appear to be quite important, especially for the most recent show years.

#### TF-IDF On answers grouped by year of air date
"""

# Performing TF-IDF on the answers
jeopardy['Answer'] = jeopardy['Answer'].astype('string')
(jeopardy.dropna(inplace = True))
yr_ans_group = jeopardy.groupby(jeopardy['Air.Date'].dt.year)['Answer'].apply(' '.join).reset_index()

yr_ans_group[0:3]

corpus3 = yr_ans_group['Answer'].tolist()

tokens2, tfidfvals2 = tfidf_vectorizer(corpus3)

idx_to_tokens2 = {}
tokens_to_idx2 = {}

for i in range(len(tokens2)):
  token = tokens2[i]
  tokens_to_idx2[token] = i
  idx_to_tokens2[i] = token

tfidfvals2.shape

for i in range(len(corpus3)):
    idx = np.argsort(tfidfvals2[i])
    idx = idx[::-1]

    print("Year: ", 1984+i)
    for j in idx[0:5]:
      print(tokens2[j])
    print("\n")

"""#### Connection between the common answers and their categories"""

top50answer = jeopardy['Answer'].value_counts()[0:50]
top50answer

mask_china = jeopardy['Answer'] == 'China'
w_mask_china = jeopardy[mask_china]
w_mask_china

high_freq_china = w_mask_china['Category'].value_counts()
high_freq_china

w_mask_china['Round'].value_counts()

high_freq_cleopatra = jeopardy[jeopardy['Answer'] == 'Cleopatra']['Category'].value_counts()
high_freq_cleopatra

high_freq_chicago = jeopardy[jeopardy['Answer'] == 'Chicago']['Category'].value_counts()
high_freq_chicago